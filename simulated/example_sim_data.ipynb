{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03569d6d56d6477",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Synthetic Dataset Analysis\n",
    "This notebook focuses on how to use both the predictive model and the conformance inference model using a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:03:32.656714Z",
     "start_time": "2024-03-03T21:03:27.810213Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from train import train_conformance_inference\n",
    "from data_sim import load_data\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8cb9a9edb241cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We initialize the main parameters used during the training of the predictive model. The model is training using a `n_fold` cross-validation approach. The number of epochs, batch size, learning rate, weight decay are set to 1000, 64, 0.0001, and 0.01, respectively. These values ought to be adjusted in accordance with the specific requirements of the problem, potentially through the application of a grid search technique.\n",
    "\n",
    "The classification model is trained with an early stop of 4 and no delta. The regression model uses an early stop of 6 with a no delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35394995442dadc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:03:32.660149Z",
     "start_time": "2024-03-03T21:03:32.657782Z"
    }
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "n_epochs = 1000\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.01\n",
    "alpha = 0.1\n",
    "patience_classification = 4\n",
    "min_delta_classification = 0.0\n",
    "patience_regression = 6\n",
    "min_delta_regression = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7741d3f8f756f82",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this instance, we employ a distinct combination of input variables, with the diabetes variable serving as the class variable. The dataset is partitioned into training and test sets in a 75/25 ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24387e4d5fbd4d08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:03:32.685442Z",
     "start_time": "2024-03-03T21:03:32.660473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/sim_1.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_data() missing 1 required positional argument: 'output_var'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m selected_combination \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX5\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# We load the simulation data using load_data auxiliary function \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_combination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# We divide data into training and test splits in a 75-25 ratio.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: load_data() missing 1 required positional argument: 'output_var'"
     ]
    }
   ],
   "source": [
    "file = os.path.join(\"../data\", \"sim_1.csv\")\n",
    "# We select the following combination of variables\n",
    "selected_combination = ['X1', 'X2', 'X3', 'X4', 'X5']\n",
    "# We load the simulation data using load_data auxiliary function \n",
    "data = load_data(file, selected_combination)\n",
    "\n",
    "# We divide data into training and test splits in a 75-25 ratio.\n",
    "n = len(data['y'])\n",
    "indices = random.sample(np.arange(n).tolist(), n)\n",
    "train_split = indices[:int((3*n) / 4)]\n",
    "test_split = indices[int((3*n) / 4):]\n",
    "\n",
    "train_data = {\n",
    "    'x': data['x'][train_split],\n",
    "    'y': data['y'][train_split],\n",
    "    'w': data['w'][train_split],\n",
    "    'z': data['z'][train_split]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b467b9a5055883",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We standardize the input data of the training set using the z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649160e7c1992c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:03:32.691155Z",
     "start_time": "2024-03-03T21:03:32.686040Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale training data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_data['x'] = scaler.fit_transform(train_data['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9c0180de3a665",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If needed, the input data of the training set can be balanced, in this case according to the output class variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93411b330a8bb61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:03:32.696874Z",
     "start_time": "2024-03-03T21:03:32.691126Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment if data balancing is needed\n",
    "\n",
    "# # Upsample unbalanced data of the training set\n",
    "# from sklearn.utils import resample\n",
    "# y = train_data['y'].reshape(-1)\n",
    "# c1 = np.argwhere(y == 1.0).reshape(-1)\n",
    "# c2 = np.argwhere(y == 0.0).reshape(-1)\n",
    "# print('C1: {}'.format(c1.shape))\n",
    "# print('C2: {}'.format(c2.shape))\n",
    "# \n",
    "# if c1.shape[0] < c2.shape[0]:\n",
    "#     c1_upsample = resample(c1, replace=True, n_samples=len(c2), random_state=42)\n",
    "#     upsampled = np.concatenate([c1_upsample, c2]).reshape(-1)\n",
    "# else: \n",
    "#     c2_upsample = resample(c2, replace=True, n_samples=len(c1), random_state=42)\n",
    "#     upsampled = np.concatenate([c1, c2_upsample]).reshape(-1)\n",
    "# print('Upsampled dataset: {}'.format(upsampled.shape))\n",
    "# \n",
    "# train_data = {\n",
    "#     'x': train_data['x'][upsampled],\n",
    "#     'y': train_data['y'][upsampled],\n",
    "#     'w': train_data['w'][upsampled],\n",
    "#     'z': train_data['z'][upsampled]\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60174c6f4be82be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The input data of the test set are also standardized using the standard scaler previously trained using the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347e272d9ec1c3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:03:32.708702Z",
     "start_time": "2024-03-03T21:03:32.695342Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = {\n",
    "    'x': data['x'][test_split],\n",
    "    'o': data['x'][test_split],\n",
    "    'y': data['y'][test_split],\n",
    "    'w': data['w'][test_split],\n",
    "    'z': data['z'][test_split]\n",
    "}\n",
    "\n",
    "# Scale test data\n",
    "test_data['x'] = scaler.transform(test_data['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04be84bc499f5f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the following snippet, we show how to train the conformance inference approach. In this example, we use an alpha value of 0.2. This parameter should be adjusted in accordance with the specific requirements of the problem, potentially through the application of a grid search technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f2818f427c152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:05:35.653836Z",
     "start_time": "2024-03-03T21:03:32.701683Z"
    }
   },
   "outputs": [],
   "source": [
    "    # We train the predictor\n",
    "predictor = train_conformance_inference(\n",
    "    data=train_data,\n",
    "    n_folds=n_folds,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    alpha=0.2,\n",
    "    patience_classification=4,\n",
    "    min_delta_classification=0.0,\n",
    "    patience_regression=6,\n",
    "    min_delta_regression=0.0,\n",
    "    hidden_sizes_bc=[64],\n",
    "    hidden_sizes_rr=[64]\n",
    ")\n",
    "\n",
    "# We test the predictor\n",
    "ci_result, ci_correct = predictor.classify(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1b215cca3c2d2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The conformance inference approach can be used to predict the diabetes risk of patients, although the predictive model is trained with only the 33% of data. Results are returned in the `y` field of the `ci_result` variable. The function `compute_classification_metrics` can be used to compute the area under the curve (`auc`), the accuracy, the recall, precision, f1-score, confusion matrix (`cm`), and cross entropy (`ce`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f04eee1404f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:05:35.861147Z",
     "start_time": "2024-03-03T21:05:35.703991Z"
    }
   },
   "outputs": [],
   "source": [
    "from metrics import compute_classification_metrics\n",
    "\n",
    "compute_classification_metrics(test_data['w'], ci_result['y'], test_data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986d0826e9e7b89",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The predictive model can also be trained independently as follows, in this case, using the complete training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c570e41f92d1bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:14:07.252133Z",
     "start_time": "2024-03-03T21:06:17.631804Z"
    }
   },
   "outputs": [],
   "source": [
    "from train_bc import cv_loop_bc\n",
    "from sklearn.model_selection import KFold\n",
    "from metrics import compute_classification_metrics\n",
    "\n",
    "k_fold = KFold(n_splits=n_folds, shuffle=False)\n",
    "indexes = sorted(range(len(train_data['y']) - 1))\n",
    "splits = k_fold.split(indexes)\n",
    "\n",
    "model_ce, metrics_ce = cv_loop_bc(\n",
    "    data=train_data,\n",
    "    splits=splits,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    patience=patience_classification,\n",
    "    min_delta=min_delta_classification,\n",
    "    hidden_sizes=[64]\n",
    "    # hidden_sizes=[10, 5, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc6cc337ec4952",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:14:07.541215Z",
     "start_time": "2024-03-03T21:14:07.493314Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics_ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786329db53b1a0e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following snippet shows how to invoke the predictive model with the test set. Note that `y` field of the test set is not used during the prediction, it is only used for computing the error and performance metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cce54fcc295cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:14:07.461278Z",
     "start_time": "2024-03-03T21:14:07.253829Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ce.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "with torch.no_grad():\n",
    "    yp = model_ce(torch.from_numpy(test_data['x']).to(device)).cpu().detach().numpy()\n",
    "\n",
    "compute_classification_metrics(test_data['w'], yp, test_data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804bc194038edc9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the following snippet we show the test dataset concatenated with the results obtained by the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67705aed618e1c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-03T21:14:07.540755Z",
     "start_time": "2024-03-03T21:14:07.469803Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(test_data['o'])\n",
    "df.columns = selected_combination\n",
    "df['Prob.'] = ci_result['p']\n",
    "df['Output'] = ci_result['y']\n",
    "df['CI'] = ci_result['ci']\n",
    "df['Target'] = np.int32(test_data['y'])\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
